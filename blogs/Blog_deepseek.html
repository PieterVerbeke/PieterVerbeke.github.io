<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Future of AI - Blog Post</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" integrity="sha384-JcKb8q3iqJ61gNV9KGb8thSsNjpSL0n8PARn9HuZOnIxN0hoP+VmmDGMN5t9UJ0Z" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha384-B4gt1jrGC7Jh4AgTPSdUtOBvfO8shuf57BaghqFfPlYxofvL8/KUEfYiJOMMV+rV" crossorigin="anonymous"></script>
    <script src="https://kit.fontawesome.com/ec75bccfcd.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="blog-style.css">
</head>

<body id="pagetop">
    <nav class="navbar navbar-expand-lg navbar-dark bg-secondary fixed-top" id="sideNav" style="min-height: 75px; padding-left:50px; padding-top: 15px; background-color:#CD5C5C;">
      <a class="navbar-brand" href=#pagetop>
        <span class="d-block d-lg-none">Pieter Verbeke</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="../index.html" style="color: #FFFFFF;">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../Academic work.html" style="color: #FFFFFF;">Academic work</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../Applied projects.html" style="color: #FFFFFF;">Applied projects</a>
          </li>
          <li class="nav-item active"><a class="nav-link" href="../Professional experience.html">Professional experience</a></li>
          <li class="nav-item active"><a class="nav-link" href="../Blogs.html">Blogs</a></li>
        </ul>
      </div>
    </nav>

    <div class="container" style="background-color:#FFEBCD;">
        <h1 style = "color:inherit; text-align:center; padding-top: 125px;">The Future of AI: Modularity and Hierarchy as Key Principles for Efficient (Artificial) Intelligence</h1>
        <p class="tags">#AI #CognitiveScience #Modularity #HierarchicalProcessing #MixtureOfExperts</p>

        <article>
            <p><a href ="https://doi.org/10.48550/arXiv.1706.03762" >"Attention is all you need."</a> That was the title of the paper proposing the algorithm behind the massive breakthrough in generative AI. However, as both humans and AI researchers have realized, paying attention requires energy.</p>
            
            <p>Until recently, the dominant approach in AI was to tackle this challenge brute-force by simply increasing computational power. Then came DeepSeek, a new open-source chatbot from China that has gained attention for its remarkable efficiency. A key factor in DeepSeek’s success is its reliance on a <strong>Mixture of Experts (MoE)</strong> architecture, an approach that delivers impressive performance while significantly reducing computational costs.</p>
            
            <h2>What Makes MoE So Powerful?</h2>
            <p>Imagine solving a complex problem by assembling a team of specialists, each with expertise in a specific domain. This is the essence of MoE. Instead of relying on a single massive model to handle every aspect of a problem, MoE distributes tasks among multiple specialized networks (experts). A gating network then routes input to the most relevant expert(s) for efficient processing.</p>
            
            <p>Advantages of MoE:</p>
            <ul>
                <li><strong>Efficiency:</strong> Instead of activating all model parameters for every task, only the relevant experts are used, significantly reducing computational costs.</li>
                <li><strong>Scalability:</strong> More experts can be added to address new problems without dramatically increasing resource demands.</li>
                <li><strong>Task-Specific Precision:</strong> Each expert specializes in a particular domain, enhancing accuracy and adaptability.</li>
            </ul>

            <h2>How Understanding the Brain Can Help Us Build Better AI</h2>
            <p>As a cognitive neuroscientist turned AI researcher, I find these developments particularly exciting because they align with what we know about human intelligence. Transformer models like ChatGPT already borrow concepts from cognitive science, particularly attention mechanisms. However, DeepSeek’s MoE architecture further reinforces what we argued in <a href="https://doi.org/10.1016/j.cobeha.2024.101374"> a recent paper </a>: efficient intelligence requires two key principles—<strong>hierarchy and modularity</strong>.</p>
            
            <p>Modularity breaks down complex problems into smaller, manageable components, while hierarchy allows for flexible recombination of these modules when needed. These principles not only form the foundation of MoE but also underpin human learning and brain function.</p>
            
            <p>Humans naturally decompose complex tasks into smaller components. For example, when learning to play the piano, we don’t start by mastering an entire composition. Instead, we first learn to read musical notes, coordinate hand movements, and understand rhythm. Later, these skills can be repurposed for other musical instruments, composition, or even understanding mathematical patterns in music. This ability to recombine knowledge across different domains is a hallmark of human cognition.</p>

            <p>Of course effective recombination of knowledge modules requires a hierarchical control process. At a high level, we make strategic decisions about which cognitive strategies to use (e.g., choosing an instrument), while lower-level processes execute specific actions (e.g., coordinating hand movements). In the human brain, the prefrontal cortex acts as the controller, functioning as a gating mechanism for selecting the most relevant cognitive modules.</p>

            <h2>Implications for the Future of AI</h2>
            <ol>
                <li><strong>More Efficient AI Models</strong> – By adopting modular and hierarchical architectures, AI can achieve greater efficiency, making large-scale models more accessible and sustainable.</li>
                <li><strong>Improved Human-AI Interaction</strong> – AI systems that mimic human cognition will be more transparent, interpretable, and adaptable to user needs.</li>
            </ol>

            <h2>Bridging the Gap Between AI and Neuroscience</h2>
            <p>Recent advances in AI have reinforced what I consider to be a fundamental message: the best AI models often mirror human intelligence. DeepSeek’s MoE architecture is a step in this direction, proving that AI can benefit from the same principles of hierarchy and modularity that enable human learning and adaptation.</p>
            
            <p>As AI continues to advance, we should not only focus on how it can assist us but also on how studying human cognition can inspire better AI <a href = "https://books.google.be/books/about/Natural_General_Intelligence.html?id=VrtgzwEACAAJ&redir_esc=y">(see also this exciting book of Christopher Summerfield)</a>. The interplay between these two fields holds the key to the next generation of intelligent systems—ones that are not just powerful but also intuitive, efficient, and deeply aligned with human needs.</p>
        </article>
    </div>
</body>
</html>